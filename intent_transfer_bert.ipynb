{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaConfig,AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transfer_bert import PhoBert_transform\n",
    "from model import LSTM_fixed_len\n",
    "from train import *\n",
    "from utils import ReviewsDataset\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBert_transform(tokenizer,phobert,max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/taindp/Jupyter/intent_bert/data'\n",
    "model_path = '/home/taindp/Jupyter/intent_bert/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_synth = pd.read_csv(os.path.join(data_path,'train_intent_synth.csv'))\n",
    "question_rm_sw = pd.read_csv(os.path.join(data_path,'train_intent_rm_sw.csv'))\n",
    "question = pd.concat([question_synth,question_rm_sw],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question['length'] = [len(item) for item in list(question['content'])]\n",
    "question['num_word'] = [len(item.split(' ')) for item in list(question['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([132., 156.,  68.,  47.,  32.,   9.,   5.,   3.,   4.,   9.]),\n",
       " array([  5. ,  16.5,  28. ,  39.5,  51. ,  62.5,  74. ,  85.5,  97. ,\n",
       "        108.5, 120. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(question['num_word']), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 465/465 [00:36<00:00, 12.65it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_emb_vector = []\n",
    "for sent in tqdm(list(question['content']),total = len(list(question['content']))):\n",
    "    input_ids = model.tokenizer_list_sentences([sent], max_length)\n",
    "    embedding = model.embedding_list_token(input_ids)\n",
    "    list_emb_vector.append(embedding)\n",
    "question['emb_vector'] = list_emb_vector\n",
    "torch.save(question,os.path.join(data_path,'trainset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = torch.load(os.path.join(data_path,'trainset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(question['emb_vector'])\n",
    "y = list(question['label'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',np.unique(y).tolist(),y)\n",
    "# class_weights = torch.tensor(class_weights,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_ds:\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_fixed_len(\n",
       "  (lstm): LSTM(768, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fixed =  LSTM_fixed_len(\n",
    "                           embedding_dim = 768,\\\n",
    "                           hidden_dim = 100,\\\n",
    "                           num_layers = 2, \\\n",
    "                           bidirectional=True,\\\n",
    "                           dropout=0.5)\n",
    "model_fixed.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_fixed.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\tTrain Loss: 0.673 | Train Acc: 65.59%\n",
      "\t Val. Acc: 67.74%\n",
      "==================================================\n",
      "epoch 1\n",
      "\tTrain Loss: 0.624 | Train Acc: 67.20%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 2\n",
      "\tTrain Loss: 0.637 | Train Acc: 67.47%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 3\n",
      "\tTrain Loss: 0.648 | Train Acc: 68.01%\n",
      "\t Val. Acc: 73.12%\n",
      "==================================================\n",
      "epoch 4\n",
      "\tTrain Loss: 0.607 | Train Acc: 66.94%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 5\n",
      "\tTrain Loss: 0.631 | Train Acc: 66.40%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 6\n",
      "\tTrain Loss: 0.577 | Train Acc: 70.16%\n",
      "\t Val. Acc: 76.34%\n",
      "==================================================\n",
      "epoch 7\n",
      "\tTrain Loss: 0.623 | Train Acc: 70.43%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 8\n",
      "\tTrain Loss: 0.590 | Train Acc: 69.62%\n",
      "\t Val. Acc: 73.12%\n",
      "==================================================\n",
      "epoch 9\n",
      "\tTrain Loss: 0.547 | Train Acc: 72.58%\n",
      "\t Val. Acc: 76.34%\n",
      "==================================================\n",
      "epoch 10\n",
      "\tTrain Loss: 0.557 | Train Acc: 71.77%\n",
      "\t Val. Acc: 81.72%\n",
      "==================================================\n",
      "epoch 11\n",
      "\tTrain Loss: 0.572 | Train Acc: 73.12%\n",
      "\t Val. Acc: 76.34%\n",
      "==================================================\n",
      "epoch 12\n",
      "\tTrain Loss: 0.531 | Train Acc: 74.46%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 13\n",
      "\tTrain Loss: 0.561 | Train Acc: 75.00%\n",
      "\t Val. Acc: 78.49%\n",
      "==================================================\n",
      "epoch 14\n",
      "\tTrain Loss: 0.559 | Train Acc: 73.92%\n",
      "\t Val. Acc: 74.19%\n",
      "==================================================\n",
      "epoch 15\n",
      "\tTrain Loss: 0.547 | Train Acc: 74.19%\n",
      "\t Val. Acc: 73.12%\n",
      "==================================================\n",
      "epoch 16\n",
      "\tTrain Loss: 0.507 | Train Acc: 73.66%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 17\n",
      "\tTrain Loss: 0.547 | Train Acc: 73.66%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 18\n",
      "\tTrain Loss: 0.524 | Train Acc: 74.46%\n",
      "\t Val. Acc: 80.65%\n",
      "==================================================\n",
      "epoch 19\n",
      "\tTrain Loss: 0.523 | Train Acc: 76.61%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 20\n",
      "\tTrain Loss: 0.512 | Train Acc: 75.00%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n",
      "epoch 21\n",
      "\tTrain Loss: 0.512 | Train Acc: 72.31%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 22\n",
      "\tTrain Loss: 0.456 | Train Acc: 78.76%\n",
      "\t Val. Acc: 78.49%\n",
      "==================================================\n",
      "epoch 23\n",
      "\tTrain Loss: 0.466 | Train Acc: 78.76%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 24\n",
      "\tTrain Loss: 0.497 | Train Acc: 79.30%\n",
      "\t Val. Acc: 80.65%\n",
      "==================================================\n",
      "epoch 25\n",
      "\tTrain Loss: 0.482 | Train Acc: 76.88%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 26\n",
      "\tTrain Loss: 0.466 | Train Acc: 79.03%\n",
      "\t Val. Acc: 80.65%\n",
      "==================================================\n",
      "epoch 27\n",
      "\tTrain Loss: 0.488 | Train Acc: 77.69%\n",
      "\t Val. Acc: 80.65%\n",
      "==================================================\n",
      "epoch 28\n",
      "\tTrain Loss: 0.486 | Train Acc: 76.88%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 29\n",
      "\tTrain Loss: 0.455 | Train Acc: 80.11%\n",
      "\t Val. Acc: 81.72%\n",
      "==================================================\n",
      "epoch 30\n",
      "\tTrain Loss: 0.492 | Train Acc: 76.61%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n",
      "epoch 31\n",
      "\tTrain Loss: 0.417 | Train Acc: 79.57%\n",
      "\t Val. Acc: 86.02%\n",
      "==================================================\n",
      "epoch 32\n",
      "\tTrain Loss: 0.473 | Train Acc: 79.84%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n",
      "epoch 33\n",
      "\tTrain Loss: 0.462 | Train Acc: 76.88%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 34\n",
      "\tTrain Loss: 0.437 | Train Acc: 79.30%\n",
      "\t Val. Acc: 80.65%\n",
      "==================================================\n",
      "epoch 35\n",
      "\tTrain Loss: 0.438 | Train Acc: 78.49%\n",
      "\t Val. Acc: 72.04%\n",
      "==================================================\n",
      "epoch 36\n",
      "\tTrain Loss: 0.447 | Train Acc: 77.96%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 37\n",
      "\tTrain Loss: 0.453 | Train Acc: 77.69%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n",
      "epoch 38\n",
      "\tTrain Loss: 0.433 | Train Acc: 80.11%\n",
      "\t Val. Acc: 79.57%\n",
      "==================================================\n",
      "epoch 39\n",
      "\tTrain Loss: 0.457 | Train Acc: 78.23%\n",
      "\t Val. Acc: 73.12%\n",
      "==================================================\n",
      "epoch 40\n",
      "\tTrain Loss: 0.425 | Train Acc: 77.96%\n",
      "\t Val. Acc: 83.87%\n",
      "==================================================\n",
      "epoch 41\n",
      "\tTrain Loss: 0.437 | Train Acc: 76.88%\n",
      "\t Val. Acc: 78.49%\n",
      "==================================================\n",
      "epoch 42\n",
      "\tTrain Loss: 0.480 | Train Acc: 79.03%\n",
      "\t Val. Acc: 81.72%\n",
      "==================================================\n",
      "epoch 43\n",
      "\tTrain Loss: 0.429 | Train Acc: 78.76%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n",
      "epoch 44\n",
      "\tTrain Loss: 0.433 | Train Acc: 77.15%\n",
      "\t Val. Acc: 78.49%\n",
      "==================================================\n",
      "epoch 45\n",
      "\tTrain Loss: 0.442 | Train Acc: 79.30%\n",
      "\t Val. Acc: 75.27%\n",
      "==================================================\n",
      "epoch 46\n",
      "\tTrain Loss: 0.450 | Train Acc: 81.72%\n",
      "\t Val. Acc: 78.49%\n",
      "==================================================\n",
      "epoch 47\n",
      "\tTrain Loss: 0.413 | Train Acc: 79.84%\n",
      "\t Val. Acc: 74.19%\n",
      "==================================================\n",
      "epoch 48\n",
      "\tTrain Loss: 0.456 | Train Acc: 77.15%\n",
      "\t Val. Acc: 82.80%\n",
      "==================================================\n",
      "epoch 49\n",
      "\tTrain Loss: 0.442 | Train Acc: 79.57%\n",
      "\t Val. Acc: 77.42%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "acc_max = 0\n",
    "for epoch in range(50):\n",
    "    train_loss, train_acc = train_model(model_fixed,train_dl,optimizer,criterion)\n",
    "    valid_acc,list_true,list_pred = evaluate (model_fixed, val_dl)\n",
    "    print('epoch',epoch)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('='*50)\n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    \n",
    "    checkpoint = {'model': model_fixed,\n",
    "          'state_dict': model_fixed.state_dict(),\n",
    "          'optimizer' : optimizer.state_dict()}\n",
    "    if valid_acc > acc_max:\n",
    "        acc_max = valid_acc\n",
    "        torch.save(checkpoint, os.path.join(model_path,'checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = load_checkpoint(os.path.join(model_path,'checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = 'có anh chị nào học ngành bảo dưỡng không cho em xin chút ý kiến ạ'\n",
    "test_input_ids = model.tokenizer_list_sentences([test_sent], max_length)\n",
    "test_embedding = torch.tensor(model.embedding_list_token(test_input_ids))\n",
    "test_embedding = test_embedding.type(torch.float32)\n",
    "test_embedding = torch.reshape(test_embedding,(1,1,768))\n",
    "preds = load_model(test_embedding)\n",
    "rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "rounded_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
