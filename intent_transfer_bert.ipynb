{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaConfig,AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transfer_bert import PhoBert_transform\n",
    "from model import LSTM_fixed_len\n",
    "from train import *\n",
    "from utils import ReviewsDataset\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBert_transform(tokenizer,phobert,max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/taindp/Jupyter/intent_bert/data'\n",
    "model_path = '/home/taindp/Jupyter/intent_bert/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = pd.read_csv(os.path.join(data_path,'train_intent_synth.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question['length'] = [len(item) for item in list(question['content'])]\n",
    "question['num_word'] = [len(item.split(' ')) for item in list(question['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([71., 94., 36., 41., 19.,  8.,  3.,  3.,  4.,  9.]),\n",
       " array([  9. ,  20.1,  31.2,  42.3,  53.4,  64.5,  75.6,  86.7,  97.8,\n",
       "        108.9, 120. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(question['num_word']), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:28<00:00, 10.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_emb_vector = []\n",
    "for sent in tqdm(list(question['content']),total = len(list(question['content']))):\n",
    "    input_ids = model.tokenizer_list_sentences([sent], max_length)\n",
    "    embedding = model.embedding_list_token(input_ids)\n",
    "    list_emb_vector.append(embedding)\n",
    "question['emb_vector'] = list_emb_vector\n",
    "torch.save(question,os.path.join(data_path,'trainset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = torch.load(os.path.join(data_path,'trainset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(question['emb_vector'])\n",
    "y = list(question['label'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',np.unique(y).tolist(),y)\n",
    "# class_weights = torch.tensor(class_weights,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_ds:\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_fixed_len(\n",
       "  (lstm): LSTM(768, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fixed =  LSTM_fixed_len(\n",
    "                           embedding_dim = 768,\\\n",
    "                           hidden_dim = 100,\\\n",
    "                           num_layers = 2, \\\n",
    "                           bidirectional=True,\\\n",
    "                           dropout=0.5)\n",
    "model_fixed.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_fixed.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\tTrain Loss: 0.699 | Train Acc: 70.43%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 1\n",
      "\tTrain Loss: 0.660 | Train Acc: 71.30%\n",
      "\t Val. Acc: 79.31%\n",
      "==================================================\n",
      "epoch 2\n",
      "\tTrain Loss: 0.611 | Train Acc: 72.17%\n",
      "\t Val. Acc: 67.24%\n",
      "==================================================\n",
      "epoch 3\n",
      "\tTrain Loss: 0.603 | Train Acc: 76.09%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 4\n",
      "\tTrain Loss: 0.586 | Train Acc: 71.30%\n",
      "\t Val. Acc: 67.24%\n",
      "==================================================\n",
      "epoch 5\n",
      "\tTrain Loss: 0.597 | Train Acc: 72.61%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 6\n",
      "\tTrain Loss: 0.598 | Train Acc: 72.17%\n",
      "\t Val. Acc: 79.31%\n",
      "==================================================\n",
      "epoch 7\n",
      "\tTrain Loss: 0.579 | Train Acc: 72.17%\n",
      "\t Val. Acc: 79.31%\n",
      "==================================================\n",
      "epoch 8\n",
      "\tTrain Loss: 0.610 | Train Acc: 72.17%\n",
      "\t Val. Acc: 77.59%\n",
      "==================================================\n",
      "epoch 9\n",
      "\tTrain Loss: 0.524 | Train Acc: 79.13%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 10\n",
      "\tTrain Loss: 0.508 | Train Acc: 74.35%\n",
      "\t Val. Acc: 77.59%\n",
      "==================================================\n",
      "epoch 11\n",
      "\tTrain Loss: 0.504 | Train Acc: 80.00%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 12\n",
      "\tTrain Loss: 0.503 | Train Acc: 75.65%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 13\n",
      "\tTrain Loss: 0.507 | Train Acc: 79.13%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 14\n",
      "\tTrain Loss: 0.478 | Train Acc: 76.96%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 15\n",
      "\tTrain Loss: 0.501 | Train Acc: 77.83%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 16\n",
      "\tTrain Loss: 0.515 | Train Acc: 77.39%\n",
      "\t Val. Acc: 67.24%\n",
      "==================================================\n",
      "epoch 17\n",
      "\tTrain Loss: 0.475 | Train Acc: 79.57%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 18\n",
      "\tTrain Loss: 0.511 | Train Acc: 79.57%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 19\n",
      "\tTrain Loss: 0.472 | Train Acc: 82.17%\n",
      "\t Val. Acc: 81.03%\n",
      "==================================================\n",
      "epoch 20\n",
      "\tTrain Loss: 0.441 | Train Acc: 79.57%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 21\n",
      "\tTrain Loss: 0.400 | Train Acc: 81.74%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 22\n",
      "\tTrain Loss: 0.464 | Train Acc: 80.87%\n",
      "\t Val. Acc: 68.97%\n",
      "==================================================\n",
      "epoch 23\n",
      "\tTrain Loss: 0.417 | Train Acc: 81.30%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 24\n",
      "\tTrain Loss: 0.427 | Train Acc: 81.30%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 25\n",
      "\tTrain Loss: 0.464 | Train Acc: 81.74%\n",
      "\t Val. Acc: 68.97%\n",
      "==================================================\n",
      "epoch 26\n",
      "\tTrain Loss: 0.453 | Train Acc: 82.17%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 27\n",
      "\tTrain Loss: 0.370 | Train Acc: 83.91%\n",
      "\t Val. Acc: 79.31%\n",
      "==================================================\n",
      "epoch 28\n",
      "\tTrain Loss: 0.504 | Train Acc: 80.87%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 29\n",
      "\tTrain Loss: 0.404 | Train Acc: 84.35%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 30\n",
      "\tTrain Loss: 0.460 | Train Acc: 85.65%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 31\n",
      "\tTrain Loss: 0.376 | Train Acc: 82.61%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 32\n",
      "\tTrain Loss: 0.378 | Train Acc: 83.91%\n",
      "\t Val. Acc: 77.59%\n",
      "==================================================\n",
      "epoch 33\n",
      "\tTrain Loss: 0.429 | Train Acc: 80.43%\n",
      "\t Val. Acc: 74.14%\n",
      "==================================================\n",
      "epoch 34\n",
      "\tTrain Loss: 0.366 | Train Acc: 81.30%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 35\n",
      "\tTrain Loss: 0.369 | Train Acc: 85.65%\n",
      "\t Val. Acc: 79.31%\n",
      "==================================================\n",
      "epoch 36\n",
      "\tTrain Loss: 0.389 | Train Acc: 84.78%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 37\n",
      "\tTrain Loss: 0.384 | Train Acc: 84.78%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 38\n",
      "\tTrain Loss: 0.435 | Train Acc: 80.00%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 39\n",
      "\tTrain Loss: 0.412 | Train Acc: 86.09%\n",
      "\t Val. Acc: 72.41%\n",
      "==================================================\n",
      "epoch 40\n",
      "\tTrain Loss: 0.434 | Train Acc: 84.78%\n",
      "\t Val. Acc: 77.59%\n",
      "==================================================\n",
      "epoch 41\n",
      "\tTrain Loss: 0.370 | Train Acc: 86.09%\n",
      "\t Val. Acc: 75.86%\n",
      "==================================================\n",
      "epoch 42\n",
      "\tTrain Loss: 0.372 | Train Acc: 83.91%\n",
      "\t Val. Acc: 70.69%\n",
      "==================================================\n",
      "epoch 43\n",
      "\tTrain Loss: 0.361 | Train Acc: 85.65%\n",
      "\t Val. Acc: 77.59%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "acc_max = 0\n",
    "for epoch in range(100):\n",
    "    train_loss, train_acc = train_model(model_fixed,train_dl,optimizer,criterion)\n",
    "    valid_acc,list_true,list_pred = evaluate (model_fixed, val_dl)\n",
    "    print('epoch',epoch)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('='*50)\n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    \n",
    "    checkpoint = {'model': model_fixed,\n",
    "          'state_dict': model_fixed.state_dict(),\n",
    "          'optimizer' : optimizer.state_dict()}\n",
    "    if valid_acc > acc_max:\n",
    "        acc_max = valid_acc\n",
    "        torch.save(checkpoint, os.path.join(model_path,'checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(list_true,list_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
